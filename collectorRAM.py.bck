from tools.externalConnections import ExternalConnections
from confluent_kafka import KafkaError, KafkaException
from confluent_kafka.error import ConsumeError
import json
import time
import os.path


monIp = "10.5.1.114"
monPort = 8989
monUrl = "/prom-manager"
kPort = 9092

csvfname = "dataset/dati.csv"

if os.path.isfile(csvfname):
    [root, end] = csvfname.split('.')
    for i in range (0,100):
        csvfname = root + str(i) + "." + end
        if not os.path.isfile(csvfname):
            break
print("save data on file {}".format(csvfname))


r1 = []
r2 = []

#ec = ExternalConnections('configC.conf')
ec = ExternalConnections('configC.conf')

period = 5

clean = 1
createKafka = 0
deleteKafka = 0

startScraper = 0
stopScraper = 1

vnf = "dtcontrolvnf"
nsd = "fgt-a3b70e2-6471-4f78-9d04-96e6671401f4"

topic = nsd + "_forecasting"

do_header = True

temp = {}
set_temp = {}
set_temp['node_memory_MemFree_bytes'] = False

metrics = {}
metrics['node_memory_MemFree_bytes'] = 'node_memory_MemFree_bytes{nsId=\"'+nsd+'\", forecast=\"no\", vnfdId=\"'+vnf+'\"}'

if clean:
   csv_file = open(csvfname, "w")
   csv_file.close()



if deleteKafka:
    print("deleting topic")
    ec.deleteKafkaTopic(topic)
    time.sleep(2)
if createKafka:
    print("creating topic")
    ec.createKafkaTopic(topic)
    time.sleep(2)

line = ""

if stopScraper:
    print("stop the scraper jobs")
    f = open('ids.txt', 'r')
    ids = f.readline().strip()
    idlist = ids.split(';')
    f.close()
    for jobId in idlist:
        print("stopping jobid {}".format(jobId))
        ec.stopScraperJob(jobId)
        time.sleep(2)


if startScraper:
    for metric in metrics.keys():
        if metric == "node_memory_MemFree_bytes":
            sId = ec.startScraperJob(nsid=nsd, topic=topic, vnfdid=vnf, metric=metric,
                              expression=metrics[metric], period=period)
        #print(sId)
        if line != "":
            line = line + ";"
        line = line + sId
        time.sleep(2)

if line != "":
   #print(line)
   text_file = open("ids.txt", "w")
   text_file.write(line)
   text_file.close()

def reset():
    #print("after write I reset")
    set_temp['node_memory_MemFree_bytes'] = False
    temp['memfree_v'] = []



def save_file():
    global do_header
    val = ""
    string = ""
    if do_header:
       hstring = ""
    if set_temp['node_memory_MemFree_bytes']:
        temp1 = temp.copy()
        t = temp1['time']
        string = str(t)
        if do_header:
            hstring = "time"
        robs = 0
        
        if 'memfv' in temp1.keys():
           mf = temp1['memfv'][len(temp1['memfv']-1)]
           string = string + ";" +  str(mf)
           if do_header:
              hstring = hstring + ";memory_free"
        #r1
        string = string + ";" +  str(r1)
        if do_header:
            hstring = hstring + ";r_a1"
        #r2
        string = string + ";" +  str(r2)
        if do_header:
            hstring = hstring + ";r_a2"
        #write
        csv_file = open(csvfname, "a")
        if do_header:
            hstring = hstring + "\n"
            csv_file.write(hstring)
            reset1()
        string = string + "\n"
        csv_file.write(string)
        csv_file.close()
        reset()
    #else:
    #    print(set_temp)


'''
    def inject_data3(self):
        #if self.set_temp['node_cpu_seconds_total'] and self.set_temp['rtt_latency'] and \
        #        self.set_temp['cmd_sent'] and self.set_temp['cmd_lost']:
        if self.set_temp['node_cpu_seconds_total']:
            temp1 = self.temp.copy()
            log.debug("temp1 copy \n{}".format(temp1))
            if 'cpu' in temp1.keys():
                for i in range(0, len(temp1['cpuv'])):
                    # string = string + ";" + str(temp1['cpuv'][i])
                    label = "cpu" + str(temp1['cpu'][i])
                    if label in self.data.keys():
                        if len(self.data[label]) == self.batch_size:
                            del self.data[label][0]
                            log.debug(self.instance_name + " forecasting Job, Deleting older element: \n{}".format(
                                self.data[label]))
                    else:
                        self.data[label] = []
                    if not label in self.set_t0.keys() or not self.set_t0[label]:
                        self.t0[label] = temp1['cpuv'][i]
                        self.set_t0[label] = True
                    self.data[label].append(round(temp1['cpuv'][i]-self.t0[label], 2))
                    log.debug(
                        self.instance_name + " forecasting Job, current data for {}, after the addition: \n{}".format(
                            label, self.data[label]))
                label = "r_a1"
                if label in self.data.keys():
                    if len(self.data[label]) == self.batch_size:
                        del self.data[label][0]
                else:
                    self.data[label] = []
                self.data[label].append(self.len_r1)
                log.info("{}->{}".format(label, self.data[label]))
                label = "r_a2"
                if label in self.data.keys():
                    if len(self.data[label]) == self.batch_size:
                        del self.data[label][0]
                else:
                    self.data[label] = []
                self.data[label].append(self.len_r2)
                log.info("{}->{}".format(label, self.data[label]))
            self.set_temp['node_cpu_seconds_total'] = False
            if self.save:
                self.savedata()
            #if self.producer is not None:
            #    self.kafka_send(str(self.get_forecasting_value(self.back, self.forward)))
            if self.producer is not None:
                msg = self.create_json()
                self.kafka_send(msg)

            self.temp = {}

'''


def reset1():
    global do_header
    do_header = False



def data_parser(json_data):
    loaded_json = json.loads(json_data)
    #print("New message: \n{}".format(loaded_json))
    '''
        'metric': {
                '__name__': 'node_memory_MemFree_bytes',
                'exporter': 'node_exporter',
                'forecasted': 'no',
                'instance': 'dtcontrolvnf-1',
                'job': '036412b5-1fae-4c47-a46d-fe584248e6bf',
                'mode': 'idle',
                'nsId': 'fgt-a3b70e2-6471-4f78-9d04-96e6671401f4'
                'vnfdId': 'dtcontrolvnf'
        },
        'value': [
                1635870778.757,
                '7001890816'
        ],
        'type_message': 'metric'
    '''
    for element in loaded_json:
       mtype = element['type_message']
       if mtype == "metric":
          m = element['metric']['__name__']
          if "MemFree" in m:
            set_temp['node_memory_MemFree_bytes'] = True
            t = element['value'][0]
            val = int(element['value'][1])
            if not 'time' in temp.keys():
               temp['time'] = t
            if not 'memfv' in temp.keys():
               temp['memfv'] = []
            temp['memfv'].append(val)
    save_file()


print("Creating the consumer")

consumer = ec.createKafkaConsumer("1", topic)

while True:
    try:
        msg = consumer.poll(1.0)
        if msg is None:
            continue
        if msg.error():
            if msg.error().code() == KafkaError._PARTITION_EOF:
                # End of partition event
                print('Forecatsing Job: %% %s [%d] reached end at offset %d\n' %
                          (msg.topic(), msg.partition(), msg.offset()))
            elif msg.error():
                raise KafkaException(msg.error())
        else:
            # no error -> message received
            data_parser(msg.value())

    except KeyboardInterrupt:
        # quit
        consumer.close()
        break
    except ConsumeError as e:
        print("Forecasting Job: Consumer error: {}".format(str(e)))
        # Should be commits manually handled?
        consumer.close()


